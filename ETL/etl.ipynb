{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Processes\n",
    "Use this notebook to develop the ETL process for each of your tables before completing the `etl.py` file to load the whole datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dropped successfully!!\n",
      "Table created successfully!!\n"
     ]
    }
   ],
   "source": [
    "%run create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=sparkifydb user=postgres password=\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath):\n",
    "    # 这将初始化一个名为 `all_files` 的空列表。\n",
    "    # 该列表将用于存储找到的所有 JSON 文件的路径。\n",
    "    all_files = []\n",
    "    # os.walk()` 函数通过自上而下或自下而上的方式生成目录树中的文件名。\n",
    "    # 对于目录树中根植于 `filepath` 的每个目录，\n",
    "    # 它都会生成一个 3 元组 `(dirpath, dirnames, filenames)`。\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        # 这一行使用 `glob` 模块搜索当前目录（`root`）中扩展名为 `.json` 的所有文件。\n",
    "        # `os.path.join(root,'*.json')` 通过将当前目录与模式 `'*.json'` 连接起来来构建路径。\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        # 嵌套的 `for` 循环（`for f in files:`）遍历在当前目录中找到的每个 JSON 文件。\n",
    "        for f in files :\n",
    "            # 使用 `os.path.abspath(f)` 确定每个文件 `f` 的绝对路径，然后将其追加到 `all_files` 列表中。\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    # 探查所有目录和子目录后，函数会返回包含所有找到的 JSON 文件绝对路径的`all_files`列表。\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `song_data`\n",
    "In this first part, you'll perform ETL on the first dataset, `song_data`, to create the `songs` and `artists` dimensional tables.\n",
    "\n",
    "Let's perform ETL on a single song file and load a single record into each table to start.\n",
    "- Use the `get_files` function provided above to get a list of all song JSON files in `data/song_data`\n",
    "- Select the first song in this list\n",
    "- Read the song file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_files = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\27117\\\\Road_To_Data_Engineering\\\\Data_Modeling_with_Postgres\\\\data\\\\song_data\\\\A\\\\A\\\\A\\\\TRAAAAW128F429D538.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = song_files[0]\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_songs</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "      <th>duration</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ARD7TVE1187B99BFB1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>California - LA</td>\n",
       "      <td>Casual</td>\n",
       "      <td>SOMZWCG12A8C13C480</td>\n",
       "      <td>I Didn't Mean To</td>\n",
       "      <td>218.93179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_songs           artist_id  artist_latitude  artist_longitude  \\\n",
       "0          1  ARD7TVE1187B99BFB1              NaN               NaN   \n",
       "\n",
       "   artist_location artist_name             song_id             title  \\\n",
       "0  California - LA      Casual  SOMZWCG12A8C13C480  I Didn't Mean To   \n",
       "\n",
       "    duration  year  \n",
       "0  218.93179     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(filepath, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1: Songs Table\n",
    "#### Extract Data for Songs Table\n",
    "- Select columns for song ID, title, artist ID, year, and duration\n",
    "- Use `df.values` to select just the values from the dataframe\n",
    "- Index to select the first (only) record in the dataframe\n",
    "- Convert the array to a list and set it to `song_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择歌曲 ID、标题、艺术家 ID、年份和持续时间列\n",
    "selected_columns = df[['song_id', 'title', 'artist_id', 'year', 'duration']] \n",
    "# 使用 df.values 只选择数据帧中的值。(values from the dataframe)\n",
    "record = selected_columns.values[0]\n",
    "# 将数组转换为列表，并将其设置为 song_data\n",
    "song_data = list(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOMZWCG12A8C13C480', \"I Didn't Mean To\", 'ARD7TVE1187B99BFB1', 0, 218.93179]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Record into Song Table\n",
    "Implement the `song_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song into the `songs` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songs` table in the sparkify database.\n",
    "\n",
    "Here you need to be careful to insert the Artist table first, otherwise the foreign key constraint will be violated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(song_table_insert, song_data)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added a record to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2: `artists` Table\n",
    "#### Extract Data for Artists Table\n",
    "- Select columns for artist ID, name, location, latitude, and longitude\n",
    "- Use `df.values` to select just the values from the dataframe\n",
    "- Index to select the first (only) record in the dataframe\n",
    "- Convert the array to a list and set it to `artist_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARD7TVE1187B99BFB1', 'Casual', 'California - LA', nan, nan]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择歌曲 ID、标题、艺术家 ID、年份和持续时间列\n",
    "selected_columns = df[['artist_id','artist_name','artist_location','artist_latitude','artist_longitude']] \n",
    "# 使用 df.values 只选择数据帧中的值。(values from the dataframe)\n",
    "record = selected_columns.values[0]\n",
    "# 将数组转换为列表，并将其设置为 song_data\n",
    "artist_data = list(record) \n",
    "artist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Record into Artist Table\n",
    "Implement the `artist_table_insert` query in `sql_queries.py` and run the cell below to insert a record for this song's artist into the `artists` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `artists` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(artist_table_insert, artist_data)\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    conn.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added a record to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process `log_data`\n",
    "In this part, you'll perform ETL on the second dataset, `log_data`, to create the `time` and `users` dimensional tables, as well as the `songplays` fact table.\n",
    "\n",
    "Let's perform ETL on a single log file and load a single record into each table.\n",
    "- Use the `get_files` function provided above to get a list of all log JSON files in `data/log_data`\n",
    "- Select the first log file in this list\n",
    "- Read the log file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = get_files(r\"C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\log_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = log_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Walter</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>Frye</td>\n",
       "      <td>NaN</td>\n",
       "      <td>free</td>\n",
       "      <td>San Francisco-Oakland-Hayward, CA</td>\n",
       "      <td>GET</td>\n",
       "      <td>Home</td>\n",
       "      <td>1540919166796</td>\n",
       "      <td>38</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1541105830796</td>\n",
       "      <td>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Summers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>GET</td>\n",
       "      <td>Home</td>\n",
       "      <td>1540344794796</td>\n",
       "      <td>139</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106106796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Des'ree</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Summers</td>\n",
       "      <td>246.30812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1540344794796</td>\n",
       "      <td>139</td>\n",
       "      <td>You Gotta Be</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106106796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist       auth firstName gender  itemInSession lastName     length  \\\n",
       "0     None  Logged In    Walter      M              0     Frye        NaN   \n",
       "1     None  Logged In    Kaylee      F              0  Summers        NaN   \n",
       "2  Des'ree  Logged In    Kaylee      F              1  Summers  246.30812   \n",
       "\n",
       "  level                           location method      page   registration  \\\n",
       "0  free  San Francisco-Oakland-Hayward, CA    GET      Home  1540919166796   \n",
       "1  free        Phoenix-Mesa-Scottsdale, AZ    GET      Home  1540344794796   \n",
       "2  free        Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong  1540344794796   \n",
       "\n",
       "   sessionId          song  status             ts  \\\n",
       "0         38          None     200  1541105830796   \n",
       "1        139          None     200  1541106106796   \n",
       "2        139  You Gotta Be     200  1541106106796   \n",
       "\n",
       "                                           userAgent  userId  \n",
       "0  \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4...      39  \n",
       "1  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       8  \n",
       "2  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       8  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(filepath, lines=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3: `time` Table\n",
    "#### Extract Data for Time Table\n",
    "- Filter records by `NextSong` action\n",
    "- Convert the `ts` timestamp column to datetime\n",
    "  - Hint: the current timestamp is in milliseconds\n",
    "- Extract the timestamp, hour, day, week of year, month, year, and weekday from the `ts` column and set `time_data` to a list containing these values in order\n",
    "  - Hint: use pandas' [`dt` attribute](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.html) to access easily datetimelike properties.\n",
    "- Specify labels for these columns and set to `column_labels`\n",
    "- Create a dataframe, `time_df,` containing the time data for this file by combining `column_labels` and `time_data` into a dictionary and converting this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Des'ree</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Summers</td>\n",
       "      <td>246.30812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1540344794796</td>\n",
       "      <td>139</td>\n",
       "      <td>You Gotta Be</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106106796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mr Oizo</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Summers</td>\n",
       "      <td>144.03873</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1540344794796</td>\n",
       "      <td>139</td>\n",
       "      <td>Flat 55</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106352796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tamba Trio</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Kaylee</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>Summers</td>\n",
       "      <td>177.18812</td>\n",
       "      <td>free</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1540344794796</td>\n",
       "      <td>139</td>\n",
       "      <td>Quem Quiser Encontrar O Amor</td>\n",
       "      <td>200</td>\n",
       "      <td>1541106496796</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       artist       auth firstName gender  itemInSession lastName     length  \\\n",
       "2     Des'ree  Logged In    Kaylee      F              1  Summers  246.30812   \n",
       "4     Mr Oizo  Logged In    Kaylee      F              3  Summers  144.03873   \n",
       "5  Tamba Trio  Logged In    Kaylee      F              4  Summers  177.18812   \n",
       "\n",
       "  level                     location method      page   registration  \\\n",
       "2  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong  1540344794796   \n",
       "4  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong  1540344794796   \n",
       "5  free  Phoenix-Mesa-Scottsdale, AZ    PUT  NextSong  1540344794796   \n",
       "\n",
       "   sessionId                          song  status             ts  \\\n",
       "2        139                  You Gotta Be     200  1541106106796   \n",
       "4        139                       Flat 55     200  1541106352796   \n",
       "5        139  Quem Quiser Encontrar O Amor     200  1541106496796   \n",
       "\n",
       "                                           userAgent  userId  \n",
       "2  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       8  \n",
       "4  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       8  \n",
       "5  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...       8  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter records by NextSong action\n",
    "df = df[df['page'] == 'NextSong']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2018-11-01 20:57:10.796\n",
       "1   2018-11-01 21:01:46.796\n",
       "2   2018-11-01 21:01:46.796\n",
       "3   2018-11-01 21:02:12.796\n",
       "4   2018-11-01 21:05:52.796\n",
       "Name: ts, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the ts timestamp column to datetime\n",
    "# Note: The current timestamp is in milliseconds, so you'll need to divide by 1000\n",
    "t = pd.to_datetime(df['ts'], unit='ms')\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timestamp, hour, day, week of year, month, year, and weekday from the ts column\n",
    "time_data = (t, t.dt.hour, t.dt.day, t.dt.isocalendar().week, t.dt.month, t.dt.year, t.dt.weekday)\n",
    "column_labels = ('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-01 20:57:10.796</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-01 21:01:46.796</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-01 21:01:46.796</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-11-01 21:02:12.796</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-11-01 21:05:52.796</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               start_time  hour  day  week  month  year  weekday\n",
       "0 2018-11-01 20:57:10.796    20    1    44     11  2018        3\n",
       "1 2018-11-01 21:01:46.796    21    1    44     11  2018        3\n",
       "2 2018-11-01 21:01:46.796    21    1    44     11  2018        3\n",
       "3 2018-11-01 21:02:12.796    21    1    44     11  2018        3\n",
       "4 2018-11-01 21:05:52.796    21    1    44     11  2018        3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe, time_df, containing the time data for this file\n",
    "time_df = pd.DataFrame(dict(zip(column_labels, time_data)))\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Time Table\n",
    "Implement the `time_table_insert` query in `sql_queries.py` and run the cell below to insert records for the timestamps in this log file into the `time` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `time` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in time_df.iterrows():\n",
    "    cur.execute(time_table_insert, list(row))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4: `users` Table\n",
    "#### Extract Data for Users Table\n",
    "- Select columns for user ID, first name, last name, gender and level and set to `user_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName',\n",
      "       'length', 'level', 'location', 'method', 'page', 'registration',\n",
      "       'sessionId', 'song', 'status', 'ts', 'userAgent', 'userId'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>Walter</td>\n",
       "      <td>Frye</td>\n",
       "      <td>M</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id first_name last_name gender level\n",
       "0       39     Walter      Frye      M  free"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = df[['userId','firstName','lastName','gender','level']] \n",
    "record = selected_columns.values[0]\n",
    "user_df = pd.DataFrame([record], columns=['user_id', 'first_name', 'last_name', 'gender', 'level'])\n",
    "user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Records into Users Table\n",
    "Implement the `user_table_insert` query in `sql_queries.py` and run the cell below to insert records for the users in this log file into the `users` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `users` table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in user_df.iterrows():\n",
    "    cur.execute(user_table_insert, row)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5: `songplays` Table\n",
    "#### Extract Data and Songplays Table\n",
    "This one is a little more complicated since information from the songs table, artists table, and original log file are all needed for the `songplays` table. Since the log file does not specify an ID for either the song or the artist, you'll need to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.\n",
    "- Implement the `song_select` query in `sql_queries.py` to find the song ID and artist ID based on the title, artist name, and duration of a song.\n",
    "- Select the timestamp, user ID, level, song ID, artist ID, session ID, location, and user agent and set to `songplay_data`\n",
    "\n",
    "#### Insert Records into Songplays Table\n",
    "- Implement the `songplay_table_insert` query and run the cell below to insert records for the songplay actions in this log file into the `songplays` table. Remember to run `create_tables.py` before running the cell below to ensure you've created/resetted the `songplays` table in the sparkify database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些准备工作\n",
    "- 因为前面做的只是一条数据的示例。现在尝试一次性把所有的相关数据插入PostgreSQL数据库中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_file(cur, filepath):\n",
    "    \"\"\"\n",
    "    Process songs files and insert records into the Postgres database.\n",
    "    :param cur: cursor reference 这是 PostgreSQL 数据库的游标引用，允许函数执行 SQL 命令。\n",
    "    :param filepath: complete file path for the file to load 这是需要处理的歌曲文件的路径。\n",
    "    \"\"\"\n",
    "    # 这个名为 \"process_song_file \"的函数用于处理单个歌曲文件，并将相关数据插入 PostgreSQL 数据库。下面是该函数的具体操作步骤：\n",
    "    \n",
    "    # 此行将 JSON 格式的歌曲文件读入 pandas DataFrame。文件以系列（`typ='series'`）的形式读取，因为它预计只包含一条记录。然后，该系列被包裹在一个列表中，以将其转换为 DataFrame。\n",
    "    # open song file\n",
    "    df = pd.DataFrame([pd.read_json(filepath, typ='series', convert_dates=False)])\n",
    "    \n",
    "    # 循环遍历 DataFrame 中的记录。由于 DataFrame 预计只包含一行（来自一个歌曲文件），因此该循环将运行一次。\n",
    "    for value in df.values:\n",
    "        # \"开头的行从 DataFrame 行中提取单个数据点，并将它们赋值给相应的变量。\n",
    "        num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year = value\n",
    "\n",
    "        # insert artist record\n",
    "        # 将艺术家数据结构化为元组，然后使用 `artist_table_insert` SQL 命令将其插入数据库。\n",
    "        artist_data = (artist_id, artist_name, artist_location, artist_latitude, artist_longitude)\n",
    "        cur.execute(artist_table_insert, artist_data)\n",
    "\n",
    "        # insert song record\n",
    "        # 同样，歌曲数据被结构化为一个元组，然后使用`song_table_insert` SQL 命令插入数据库。\n",
    "        song_data = (song_id, title, artist_id, year, duration)\n",
    "        cur.execute(song_table_insert, song_data)\n",
    "    \n",
    "    print(f\"Records inserted for file {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_file(cur, filepath):\n",
    "    \"\"\"\n",
    "    Process Event log files and insert records into the Postgres database.\n",
    "    :param cur: cursor reference\n",
    "    :param filepath: complete file path for the file to load\n",
    "    \"\"\"\n",
    "    # open log file\n",
    "    # 该行将 JSON 格式的日志文件读入 pandas DataFrame。\n",
    "    # `lines=True` 参数表示文件每行有一个 JSON 对象。\n",
    "    df = pd.read_json(filepath, lines=True)\n",
    "    \n",
    "    # 通过 `NextSong` 操作进行过滤：\n",
    "    #这将过滤 DataFrame，使其只包含`page`列的值为 \"NextSong \"的记录。\n",
    "    #它还将 `ts` 列转换为精度为毫秒的日期时间格式。\n",
    "    # filter by NextSong action\n",
    "    df = df[df['page'] == \"NextSong\"].astype({'ts': 'datetime64[ms]'})\n",
    "\n",
    "    # 将时间戳列转换为日期时间格式\n",
    "    # convert timestamp column to datetime\n",
    "    t = pd.Series(df['ts'], index=df.index)\n",
    "\n",
    "    # insert time data records\n",
    "    # 代码会构建一个新的 DataFrame `time_df`，将时间戳分解为各种时间成分，\n",
    "    # 如小时、天、年的星期等。\n",
    "    column_labels = [\"timestamp\", \"hour\", \"day\", \"weelofyear\", \"month\", \"year\", \"weekday\"]\n",
    "    time_data = (t, t.dt.hour, t.dt.day, t.dt.isocalendar().week, t.dt.month, t.dt.year, t.dt.weekday)\n",
    "\n",
    "    time_df = pd.DataFrame(dict(zip(column_labels, time_data)))\n",
    "    \n",
    "    # 然后将这些数据插入数据库的时间表中。\n",
    "    for i, row in time_df.iterrows():\n",
    "        cur.execute(time_table_insert, list(row))\n",
    "\n",
    "    # 这将创建一个新的 DataFrame `user_df`，其中包含与用户相关的列。\n",
    "    # load user table\n",
    "    user_df = df[['userId','firstName','lastName','gender','level']]\n",
    "    \n",
    "    # 然后，函数遍历 `user_df` DataFrame，并将用户数据插入数据库中的 users 表。\n",
    "    # insert user records\n",
    "    for i, row in user_df.iterrows():\n",
    "        cur.execute(user_table_insert, row)\n",
    "\n",
    "    # 函数遍历原始数据帧 `df`。对于每一行：\n",
    "    # insert songplay records\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # get songid and artistid from song and artist tables\n",
    "        # 它会根据歌曲名称、艺术家姓名和歌曲持续时间，尝试在数据库中查找匹配的歌曲和艺术家。\n",
    "        cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "        results = cur.fetchone()\n",
    "        \n",
    "        # 如果找到匹配，它将检索歌曲 ID 和艺术家 ID。\n",
    "        if results:\n",
    "            songid, artistid = results\n",
    "        else:\n",
    "            songid, artistid = None, None\n",
    "\n",
    "        # insert songplay record\n",
    "        #  然后，它会构建歌曲播放数据，并将其插入数据库中的歌曲播放表。\n",
    "        songplay_data = ( row.ts, row.userId, row.level, songid, artistid, row.sessionId, row.location, row.userAgent)\n",
    "        cur.execute(songplay_table_insert, songplay_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(cur, conn, filepath, func):\n",
    "    \"\"\"\n",
    "    Driver function to load data from songs and event log files into Postgres database.\n",
    "    :param cur: a database cursor reference 指向 PostgreSQL 数据库的游标引用，允许函数执行 SQL 命令。\n",
    "    :param conn: database connection reference 数据库连接引用，用于向数据库提交事务。\n",
    "    :param filepath: parent directory where the files exists JSON 文件所在的目录路径。\n",
    "    :param func: function to call 处理每个文件时应调用的函数的引用。根据上下文，可以是 `process_song_file` 或 `process_log_file`。\n",
    "    \"\"\"\n",
    "    # process_data \"函数是一个驱动函数，用于处理 JSON 文件（歌曲文件或事件日志文件）中的数据并将其加载到 PostgreSQL 数据库中。下面是其功能的细分：\n",
    "    # get all files matching extension from directory\n",
    "    # 该函数首先初始化一个空列表 `all_files`。然后，它从 `filepath` 开始遍历所有目录和子目录，并收集找到的所有 JSON 文件。这些文件路径会追加到 `all_files` 列表中。\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    #  该函数计算并打印在指定目录中找到的 JSON 文件总数。\n",
    "    # get total number of files found\n",
    "    num_files = len(all_files)\n",
    "    print('{} files found in {}'.format(num_files, filepath))\n",
    "    \n",
    "    # 然后，函数遍历`all_files`列表中的每个文件。对于每个文件：\n",
    "    # 它调用指定的 `func`（`process_song_file` 或 `process_log_file`）来处理文件。\n",
    "    # 使用 `conn.commit()` 向数据库提交事务。\n",
    "    # 打印一条进度消息，说明总共处理了多少文件。\n",
    "    # iterate over files and process\n",
    "    for i, datafile in enumerate(all_files, 1):\n",
    "        func(cur, datafile)\n",
    "        conn.commit()\n",
    "        print('{}/{} files processed.'.format(i, num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 files found in C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAAAW128F429D538.json\n",
      "1/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAABD128F429CF47.json\n",
      "2/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAADZ128F9348C2E.json\n",
      "3/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAAEF128F4273421.json\n",
      "4/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAAFD128F92F423A.json\n",
      "5/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAAMO128F1481E7F.json\n",
      "6/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAAMQ128F1460CD3.json\n",
      "7/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAAPK128E0786D96.json\n",
      "8/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAARJ128F9320760.json\n",
      "9/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\A\\TRAAAVG12903CFA543.json\n",
      "10/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABCL128F4286650.json\n",
      "11/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABDL12903CAABBA.json\n",
      "12/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABJL12903CDCF1A.json\n",
      "13/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABJV128F1460C49.json\n",
      "14/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABLR128F423B7E3.json\n",
      "15/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABNV128F425CEE1.json\n",
      "16/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABRB128F9306DD5.json\n",
      "17/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABVM128F92CA9DC.json\n",
      "18/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABXG128F9318EBD.json\n",
      "19/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\B\\TRAABYN12903CFD305.json\n",
      "20/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACCG128F92E8A55.json\n",
      "21/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACER128F4290F96.json\n",
      "22/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACFV128F935E50B.json\n",
      "23/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACHN128F1489601.json\n",
      "24/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACIW12903CC0F6D.json\n",
      "25/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACLV128F427E123.json\n",
      "26/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACNS128F14A2DF5.json\n",
      "27/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACOW128F933E35F.json\n",
      "28/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACPE128F421C1B9.json\n",
      "29/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\A\\C\\TRAACQT128F9331780.json\n",
      "30/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABACN128F425B784.json\n",
      "31/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAFJ128F42AF24E.json\n",
      "32/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAFP128F931E9A1.json\n",
      "33/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAIO128F42938F9.json\n",
      "34/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABATO128F42627E9.json\n",
      "35/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAVQ12903CBF7E0.json\n",
      "36/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAWW128F4250A31.json\n",
      "37/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAXL128F424FC50.json\n",
      "38/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAXR128F426515F.json\n",
      "39/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\A\\TRABAXV128F92F6AE3.json\n",
      "40/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBAM128F429D223.json\n",
      "41/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBBV128F42967D7.json\n",
      "42/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBJE12903CDB442.json\n",
      "43/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBKX128F4285205.json\n",
      "44/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBLU128F93349CF.json\n",
      "45/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBNP128F932546F.json\n",
      "46/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBOP128F931B50D.json\n",
      "47/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBOR128F4286200.json\n",
      "48/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBTA128F933D304.json\n",
      "49/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\B\\TRABBVJ128F92F7EAA.json\n",
      "50/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCAJ12903CDFCC2.json\n",
      "51/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCEC128F426456E.json\n",
      "52/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCEI128F424C983.json\n",
      "53/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCFL128F149BB0D.json\n",
      "54/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCIX128F4265903.json\n",
      "55/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCKL128F423A778.json\n",
      "56/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCPZ128F4275C32.json\n",
      "57/60 files processed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCRU128F423F449.json\n",
      "58/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCTK128F934B224.json\n",
      "59/60 files processed.\n",
      "Records inserted for file C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\\A\\B\\C\\TRABCUQ128E0783E2B.json\n",
      "60/60 files processed.\n"
     ]
    }
   ],
   "source": [
    "process_data(cur, conn, filepath=r\"C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\song_data\", func=process_song_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 files found in C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\log_data\n",
      "1/30 files processed.\n",
      "2/30 files processed.\n",
      "3/30 files processed.\n",
      "4/30 files processed.\n",
      "5/30 files processed.\n",
      "6/30 files processed.\n",
      "7/30 files processed.\n",
      "8/30 files processed.\n",
      "9/30 files processed.\n",
      "10/30 files processed.\n",
      "11/30 files processed.\n",
      "12/30 files processed.\n",
      "13/30 files processed.\n",
      "14/30 files processed.\n",
      "15/30 files processed.\n",
      "16/30 files processed.\n",
      "17/30 files processed.\n",
      "18/30 files processed.\n",
      "19/30 files processed.\n",
      "20/30 files processed.\n",
      "21/30 files processed.\n",
      "22/30 files processed.\n",
      "23/30 files processed.\n",
      "24/30 files processed.\n",
      "25/30 files processed.\n",
      "26/30 files processed.\n",
      "27/30 files processed.\n",
      "28/30 files processed.\n",
      "29/30 files processed.\n",
      "30/30 files processed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    process_data(cur, conn, filepath=r\"C:\\Users\\27117\\Road_To_Data_Engineering\\Data_Modeling_with_Postgres\\data\\log_data\", func=process_log_file)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    conn.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see if you've successfully added records to this table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Connection to Sparkify Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement `etl.py`\n",
    "Use what you've completed in this notebook to implement `etl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
